1. 因为属性或者特征过多造成的问题，如果可以选择重要的特征，使得仅需要一部分特征就可以构建模型，可以大大减轻维数灾难问题，从这个意义上讲，特征选择和降维技术有相似的动机，事实上它们也是处理高维数据的两大主流技术。[特征工程](https://zhuanlan.zhihu.com/p/57356973)

2. 去掉取值变化小的特征

   假如某特征只有 0 和 1 的两种取值，并且所有输入样本中，95% 的样本的该特征取值都是 1 ，那就可以认为该特征作用不大。这个方法简单，但不太好用，可以作为特征选择的一个预处理，先去掉变化小的特征。

3. 信息增益一定是非负的。

4. 信息增益偏好取值多的属性、信息增益率偏好取值少的属性。通过设定阈值来确定两个选择的特征。