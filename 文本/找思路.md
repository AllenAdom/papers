#### 2020.01.07

1. A Style-Based Generator Architecture for Generative Adversarial Networks

   本文是StyleGAN内容，获得了CVPR 2019 最佳论文荣誉提名奖。

2. XLNet: Generalized Autoregressive Pretraining for Language Understanding

   XLNet：用于语言理解的通用自回归预训练；基于 BERT 的改进模型。

3. ALBERT: A Lite BERT for Self-supervised Learning of Language Representations

   简化版 BERT，但不是简单的缩小了事，他们用更少的参数获得了更好的表现。

4. SinGAN: Learning a Generative Model from a Single Natural Image

   这篇论文尝试从单张图像学习 GAN，多种不同尺度的 GAN 组成的金字塔结构分别学习图像中不同大小的小块，整个模型的学习效果得以同时兼顾图像中的全局结构和细节纹理。**ICCV 2019 最佳论文。**

#### 2020.01.08

1. The Illustrated Transformer [链接](https://blog.csdn.net/yujianmin1990/article/details/85221271)

   【注意：不是每个词向量独享3个matrix，而是所有输入共享3个转换矩阵；**权重矩阵是基于输入位置的转换矩阵**；有个可以尝试的点，如果每个词独享一个转换矩阵，会不会效果更厉害呢？】

#### 2020.01.09

1. 关于**Transformer**

   要和老师讨论的问题：

   1. 论文中已经提了Transformer具有很好的并行性，我们还有必要继续做并行实现吗？ 文中用了8个GPU进行加速训练还达不到集群层次，距离真正意义上的并行实现还有差距。

   2. 之前看的论文并行的是算法，而Transformer是一种架构，并行起来可能会有什么困难？

      Transformer is a new simple network architecture.

   3. Transformer是一篇2017年的论文提出来的，会不会有点老？

#### 2020.03.04

在BigGAN中得出的结论，是算力的胜利；则有如下思考：

1. 在集群中是否可以用不同数量的机器分别训练G，D？
2. 那么机器的数量该如何合理分配给G，D？
3. G、D之间有数据交互，该如何实现高效的数据交换？
4. 对G、D本身的分布式训练该如何考量？

#### 2020.03.07

读了 Multi-Discriminator Generative Adversarial Networks for Distributed Datasets 文章，该文章主要是实现了多判别器。无论是在数据集上，还是在集群数量上，还是在算法的改进上都没有明显亮点，读完该片论文有如下思考：

1. 为什么不去实现多生成器？
2. 在多判别器的情况下会不会出现D等待G的情况？
3. G直接向D传输生成的图片会不会造成数据传输量过大？
4. 多判别器对单一生成器，本文中生成器又和parameter server集成在一起，则会对生成器端造成巨大压力。
5. 本文参数的更新使用异步的方式。
6. 本文在实现多判别器的时候，采用相同的判别器架构部署到每一台机器上，数据集也是平均的划分，并没有考虑机器与机器之间硬件资源和软件资源的差异，若更合理的划分，（因为在一段时间内，两天，三天甚至更久，整个集群的状态是稳定的，即几乎保持不变，不会像我们个人PC一样），然后再去考虑该给这台机器分配多少数据集和相应的任务。

#### 2020.03.09

读了GENERATIVE ADVERSARIAL PARALLELIZATION文章

1. 本文没有在IS和FID上进行实验评估。
2. 虽说是并行，并未给出训练性能上的提升。

#### 2020.03.12

1. GAN可以生成数据，那么是否可以缓解在某些领域的数据稀疏问题？比如电子病历？

2. 从真实数据集中抽取的样本并不一定能反应真实样本的数据分布，如何解决？

   两个方面，从小样本训练出发，考虑如何通过小样本来训练出优秀的GAN；第二个方面，判别器交叉学习数据集，尽可能复现真实数据集的数据分布。

#### 2020.03.14

1. 据目前为止，看到的都可以看作是模型上的并行，都是G *vs* Ds, Gs *vs* D or Gs *vs* Ds，但是针对某一G或者D并不是并行的，都是在一台机器上训练，所以我想能不能做一种“双层并行”，上层分别训练G和D，下层训练CNN，这样在缩短训练时间的同时，还能享受Ds和Gs带来的好处。当然也可以同时考虑数据并行。
2. GAN一直被视为是G和D两者的博弈，在实际训练中经常会遇到一方过强一方过弱的现象，那么我们是否可以引入一个**调节器**，使得在训练中任何一方都得到另一方的正向反馈(positive feedback)，有助于GAN的训练。
3. 和老师交流沟通总结：
   1. 硕士首先要学会从别人已有的工作出发，在别人的工作基础之上做改进，这种改进是*微调*，并不需要做创新，把握好难度尺寸才是最重要的。
   2. 在已有工作基础上做改进有两个方面，第一个是本身算法的改进，另一个是应用背景的更换。

#### 2020.03.25

1. 在训练GAN网络的时候，我们会假设训练样本服从高斯分布，但是我们在做数据并行的时候，往往选择的是粗暴的随机的均分策略，那么我们可以针对这一点进行改进，就是能不能从训练样本选择出的子样本仍服从正态分布，这样能增强生成图片的多样性。

#### 2020.03.26

1. 既然自己对算法进行了改进，那就贡献到 Alink 或者 Spark上去！
2. Alink、Spark、Hadoop、Tensorflow都有现成随机森林库。



#### 2020.03.27

1. 我是否可以将数据存到HBase里面，陈师兄将数据存到了HDFS中。
2. 步骤：
   1. 数据预处理：降维。
   2. 选择一种数据并行模型，拟采用机器性能衡量方法。
   3. 在spark、Hadoop、tensorflow、Alink上分别运行提供的随机森林算法。
   4. 实现自己设计的随机森林算法，与之进行对比。

#### 2020.03.28

1. 特征子集的选取可以用信息增益率来选，但是要选多少个？
2. 去掉取值变化小的特征。
3. C4.5算法中有大量的重复计算，可针对其优化。

#### 2020.03.29

树模型在学习时，是以纯度为评价基准，选择最好的分裂属性进行分裂，这本身也可以看作一个特征选择的过程。这里选择随机森林为选择算法，在实现上，有平均不纯度减少和平均精确度减少两种。

**2.3.2.1 平均不纯度减少**

树模型的训练过程，总在选择最优的属性将数据分裂，属性的优劣是通过计算每个特征对树的不纯度的减少程度。而对于随机森林，可以计算**每课树减少的不纯度的平均值**，作为特征的重要性系数。

但不纯度方法存在一定的缺陷。不论哪种度量手段，都存在着一定的偏好。例如信息增益偏好取值多的属性、信息增益率偏好取值少的属性。且对于存在关联的一组强特征，率先被选择的属性重要性远远高于后被选择的属性，因为某属性一旦备选意味着数据集不纯度会迅速下降，而其他属性无法再做到这一点，容易对特征的理解产生歧义。

**2.3.2.2 平均精确率减少**

平均精确度减少是直接度量每个特征对模型精确率的影响。主要思路是打乱特征的特征值顺序，度量**顺序变动对于模型精确率**的影响。很明显，对于不重要的变量，打乱顺序对精确率影响不会太大，但重要的特征，就会对精确率产生明显的影响。

#### 2020.04.01

1. 每一棵决策树都对应一个训练集，要构建 N 棵决策树，那就需要产生对应数量 的训练集，从原始训练集中产生 N 个训练子集就涉及到统计抽样技术。
2. 传统的决策树在选择划分属性时是在当前结点的属性集合（假定有d个属性）中选择一个最优属性；而在随机森林中，对子决策树的每个结点，先从该结点的属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个最优属性用于划分。这里的参数k控制了随机性的引入程度：若令k=d，则子决策树的构建与传统决策树相同，一般情况下，推荐值k=log<sub>2</sub>d。
3. 陈论文中：所有的决策树都在master，所需要的计算信息都从slave上获取。
4. 真正决定计算量大小的是属性值的数量，数量越大，计算越耗时。

#### 2020.04.02

1. 假如数据集过大，可以将分析限制为依次分析固定大小子集的流方法。如果能将**流处理**应用到随机森林算法中就太好了。

2. IVoting produces more accurate ensembles than bagging in distributed settings

   Learning ensembles from bites: A scalable and accurate approach

3. 决策树算法应用及并行化研究_李伟中使用洛必达法则对对数计算做了优化，还有人提到可以用泰勒公式进行优化。

#### 2020.04.03

1. 特征选择方法变化，由原本G和GR变为取 top G加上随机的(m-k)个。

#### 2020.04.06

1. 在《面向高维特征和多分类的分布式梯度提升树》中提到的数据集转置方法可以参考，实际上就是将数据集行数据存储为列数据，用于特征并行。

#### 2020.04.06

1. 之前提到过的C4.5算法优化，可以将ln2在最后计算，避免每个元素都要乘以ln2.



#### 2020.05.03

1. DPOCSS算法使用特征并行

   考虑两点：机器性能；内存容量，即分配给的矩阵过大，内存溢出怎么办

2. 演化算法中对种群的评价算法计算复杂度仍较高(O(mn))，是否可以进一步优化？

3. 最后评价算法，仍需要整个矩阵参与计算，是否存在单机无法满足问题？

   