#### 本周工作总结

1. 初步拟定了研究方向----基于Flink的并行优化随机森林算法

2. 初步计划了**工作步骤** 和 如何实现的**最初思路**

3. 调研了随机森林算法及其部分优化算法。

   但是目前调研的优化算法大多没有考虑到维数灾难，提出的算法虽能提高分类的准确率，但针对高维数据来说，计算量过大。有部分是面向大数据的文献，但是因为或不清楚期刊质量、或没能挖掘到该文献对我有用的点。所以调研这些文献后增广了我对随机森林算法优化的知识面。

4. 对数据预处理和特征选择做了调查

   * 数据预处理方面：去掉取值变化小的特征

   * 特征选择

     受陈论文中的数据降维启发，提出基于信息增益和信息增益率混合选取的特征选择方法，下对该方法进行说明：

     1）背景

     随机森林核心实现仍是决策树，决策树中最重要的算法是节点分裂算法，C4.5是针对ID3（一种基于**信息增益**的特征选择算法）的改进，是节点分裂算法的经典算法，该算法是利用**信息增益率**的大小来选取在该节点分裂的特征。陈将所有特征按照信息增益率进行降序排序，共选取m个特征(共M个，m << M)，其中取**前k个**特征，再从剩下的特征里面**随机**选取(m-k)个，其中m和k均是手动指定的。

     2）分析

     使用信息增益率来进行特征选择主要有两个好处，第一避免了额外的特征选择的计算量，只需要排序即可；第二信息增益率给特征选择提高了可信赖的理论基础，**可解释性强**。

     我觉得这种做法很好，考虑是否能从这一方法提出改进。

     3）改进方向

     从两个方面进行改进：

     * 偏向性

       经调查，有研究表明：信息增益偏好**取值多的属**性、信息增益率**偏好取值少的属性**。

     * 盲目性

       在m和k的取值上以及随机选取(m-k)个特征，这两步操作我认为都存在很大的不确定性，没有理论指导。

     4）改进

     上文在偏向性的分析中，发现信息增益和信息增益率的**偏向是互补的**，所以我们可以选择信息增益和信息增益率混合的特征选择的方法。与陈的方法相比，只是多了一个信息增益排序的过程。

     算法描述：在获得特征关于信息增益(记 集合**G**)和信息增益率(记 集合**GR**)的排序后，我们要选择m个特征，先在**GR**中选取前 *$$\frac{1}{2}$$m* 个，然后再从**G**中选取 *$$\frac{1}{2}$$m* 个特征，组成特征集合。若在**G**中选取到的特征与**GR**中选取到的特征重合，则按排序顺序取下一个即可。

     <font size=3px color="gray"> 备注：因为使用信息增益率的方法弥补了使用信息增益的一些不足，所以我们先从**GR**中选取，再从**G**中选取。</font>

     通过上述算法，只增加了一个排序就可获得更具有说服力的特征子集。避免了额外计算负担，避免了随机选取带来的盲目性，将手动指定了两个变量数目 (m、k)降低到了一个。

     5）关于改进的思考

     * 无论使用信息增益还是信息增益率，都不能挖掘特征之间的关联，率先被选择的属性重要性远远高于后被选择的属性。

     * 在此之前，我一直想找一个理论基础，用来指导m的取值，避免手动指定，需要这个理论即要计算简便，也不需要手动设定阈值，但是尚未找到。

     * 在对特征排序过程中，没有必要对全部特征都进行排序，比如在**GR**中我们我们只需要前 *$$\frac{1}{2}$$m* 个，所以只要找到前*$$\frac{1}{2}$$m* 即可。

#### 下周工作计划

​	打算关于特征选择的工作告一段落（虽然仍有尚未解决的问题），开始数据并行方法的探索。

#### 总结和思考

​	在学习过程中要多记录，记录之后以后用到了查阅起来会很方便。